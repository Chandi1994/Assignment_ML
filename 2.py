# -*- coding: utf-8 -*-
"""2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/153VS_idLc6c-ep4scRMK0qM0EkcxEe0z
"""

from keras import models, layers
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.optimizers import SGD
import matplotlib.pyplot as plt
import numpy as np

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train.shape, x_test.shape

# reshape dataset to have a single channel
x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))
x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))

#target values
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# convert from integers to floats
x_train= x_train.astype('float32')
x_test= x_test.astype('float32')

# rescale pixel values from range [0, 255] to [0, 1]
x_train = x_train / 255.0
x_test = x_test / 255.0

# Building the model
model = models.Sequential()

# 3 convolutional layers
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

# 1 hidden layers
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))

# The output layer with 10 neurons, for 10 classes
model.add(layers.Dense(10, activation='softmax'))

# Compiling the model using some basic parameters
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

noise_factor = 0.25
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 

x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

model.fit(x_train_noisy, y_train, epochs=10, batch_size=64)

test_loss, test_acc = model.evaluate(x_test_noisy, y_test)
print('Accuracy:', test_acc)
print('Loss: ', test_loss)

noise_factor = 0.4
x_train_noisy1 = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
x_test_noisy1 = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 

x_train_noisy1 = np.clip(x_train_noisy1, 0., 1.)
x_test_noisy1 = np.clip(x_test_noisy1, 0., 1.)

model.fit(x_train_noisy1, y_train, epochs=10, batch_size=64)

test_loss, test_acc = model.evaluate(x_test_noisy1, y_test)
print('Accuracy:', test_acc)
print('Loss: ', test_loss)

noise_factor = 0.1
x_train_noisy2 = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
x_test_noisy2 = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 

x_train_noisy2 = np.clip(x_train_noisy, 0., 1.)
x_test_noisy2 = np.clip(x_test_noisy, 0., 1.)

model.fit(x_train_noisy2, y_train, epochs=10, batch_size=64)

test_loss, test_acc = model.evaluate(x_test_noisy2, y_test)
print('Accuracy:', test_acc)
print('Loss: ', test_loss)