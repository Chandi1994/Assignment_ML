# -*- coding: utf-8 -*-
"""3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R5RguR9fljMApOubXJlxl4YLj69nfsJ7
"""

from keras import models, layers
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.optimizers import SGD
import matplotlib.pyplot as plt
import numpy as np

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train.shape, x_test.shape

# reshape dataset to have a single channel
x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))
x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))

#target values
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# convert from integers to floats
x_train= x_train.astype('float32')
x_test= x_test.astype('float32')

# rescale pixel values from range [0, 255] to [0, 1]
x_train = x_train / 255.0
x_test = x_test / 255.0

noise_factor = 0.25
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 

x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

# The encoding process
input_img = layers.Input(shape=(28, 28, 1))  

############
# Encoding #
############

# Conv1 #
x = layers.Conv2D(filters = 16, kernel_size = (3, 3), activation='relu', padding='same')(input_img)
x = layers.MaxPooling2D(pool_size = (2, 2), padding='same')(x)

# Conv2 #
x = layers.Conv2D(filters = 8, kernel_size = (3, 3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D(pool_size = (2, 2), padding='same')(x) 

# Conv 3 #
x = layers.Conv2D(filters = 8, kernel_size = (3, 3), activation='relu', padding='same')(x)
encoded = layers.MaxPooling2D(pool_size = (2, 2), padding='same')(x)

############
# Decoding #
############

# DeConv1
x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
x = layers.UpSampling2D((2, 2))(x)

# DeConv2
x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)
x = layers.UpSampling2D((2, 2))(x)

# Deconv3
x = layers.Conv2D(16, (3, 3), activation='relu')(x)
x = layers.UpSampling2D((2, 2))(x)
decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

autoencoder = models.Model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

autoencoder.fit(x_train_noisy, x_train, epochs=10, batch_size=128)

# decoded data

decoded_x_train = autoencoder.predict(x_train)
decoded_x_test = autoencoder.predict(x_test)

# Building the model
model = models.Sequential()

# 3 convolutional layers
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

# 1 hidden layers
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))

# The output layer with 10 neurons, for 10 classes
model.add(layers.Dense(10, activation='softmax'))

# Compiling the model using some basic parameters
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(decoded_x_train, y_train, epochs=10, batch_size=64)

test_loss, test_acc = model.evaluate(decoded_x_test, y_test)
print('Accuracy:', test_acc)
print('Loss: ', test_loss)